{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch pandas scikit-learn transformers sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pE0v9inU2k0j",
        "outputId": "e98d3e2b-1098-49d2-f244-5a1139e59380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.0)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 sentence-transformers-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Model Loading"
      ],
      "metadata": {
        "id": "W_Ls8_SqyeFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, InputExample\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from sklearn.metrics import matthews_corrcoef, roc_auc_score, confusion_matrix, classification_report, accuracy_score"
      ],
      "metadata": {
        "id": "sXxee9S4Y8Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zW1zV_P-bd_1",
        "outputId": "17fe1b12-d584-4e63-8f76-af0de9733791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Please change this when running locally\n",
        "base_url = '/content/drive/My Drive/colab-data/'"
      ],
      "metadata": {
        "id": "iMaPswDXdC2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a PyTorch Dataset for authorship verification tasks\n",
        "class AuthorshipVerificationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A dataset class that encapsulates a list of examples for authorship verification.\n",
        "    Each example consists of two texts and a binary label indicating if they are written by the same author.\n",
        "    \"\"\"\n",
        "    def __init__(self, examples):\n",
        "        \"\"\"\n",
        "        Initialize the dataset with a list of InputExamples.\n",
        "\n",
        "        Args:\n",
        "            examples (list): A list of InputExample objects.\n",
        "        \"\"\"\n",
        "        self.examples = examples\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the number of examples in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieve the InputExample at the specified index in the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx (int): The index of the example to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            InputExample: The requested example.\n",
        "        \"\"\"\n",
        "        return self.examples[idx]\n",
        "\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collation function for DataLoader that prepares batches for processing.\n",
        "    This function handles text pairs, optionally with labels if present, suitable for\n",
        "    tasks such as text similarity or classification.\n",
        "\n",
        "    Args:\n",
        "        batch (list): A list of data instances, where each instance is expected to\n",
        "                      have 'texts' (a list containing two pieces of text) and optionally 'label'.\n",
        "\n",
        "    Returns:\n",
        "        tuple: If labels are present, returns two lists of texts and a tensor of labels.\n",
        "               Otherwise, returns only the two lists of texts.\n",
        "    \"\"\"\n",
        "    # Extract the first and second texts from each item in the batch\n",
        "    texts1 = [item.texts[0] for item in batch]\n",
        "    texts2 = [item.texts[1] for item in batch]\n",
        "\n",
        "    # Check if the first item has a label attribute\n",
        "    if hasattr(batch[0], 'label'):\n",
        "        labels = [item.label for item in batch]\n",
        "        return texts1, texts2, torch.tensor(labels, dtype=torch.float)\n",
        "    else:\n",
        "        return texts1, texts2\n"
      ],
      "metadata": {
        "id": "YFckjyf7Y-TG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "model_path = base_url+'roberta/output/training_output'\n",
        "model = SentenceTransformer(model_path)"
      ],
      "metadata": {
        "id": "l0mEt_WQZSV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "W-vi8N52yuPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(filepath):\n",
        "    \"\"\"\n",
        "    Load a dataset from a specified file path and prepare it for model training or evaluation.\n",
        "    Assumes the file is in CSV format and contains text pair columns 'text_1' and 'text_2',\n",
        "    along with an optional 'label' column for supervised learning.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): The path to the CSV file containing the dataset.\n",
        "\n",
        "    Returns:\n",
        "        AuthorshipVerificationDataset: A dataset object containing preprocessed examples.\n",
        "    \"\"\"\n",
        "    dataframe = pd.read_csv(filepath)\n",
        "    dataframe.fillna(\"\", inplace=True)\n",
        "    examples = [InputExample(texts=[row['text_1'], row['text_2']], label=float(row.get('label', 0)))\n",
        "                for index, row in dataframe.iterrows()]\n",
        "    return AuthorshipVerificationDataset(examples)\n",
        "\n",
        "def predict_and_save(model, dataloader, filepath):\n",
        "    \"\"\"\n",
        "    Predict using a pre-trained SentenceTransformer model and save the predictions to a CSV file.\n",
        "    The function calculates cosine similarities between embeddings of text pairs and applies\n",
        "    a threshold to determine the binary classification outcomes.\n",
        "\n",
        "    Args:\n",
        "        model (SentenceTransformer): The pre-trained model to use for predictions.\n",
        "        dataloader (DataLoader): The DataLoader providing batches of data for prediction.\n",
        "        filepath (str): The path where the prediction results CSV will be saved.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            if len(batch) == 3:\n",
        "                texts1, texts2, _ = batch\n",
        "            else:\n",
        "                texts1, texts2 = batch\n",
        "\n",
        "            # Encode the text pairs to get their embeddings\n",
        "            embeddings1 = model.encode(texts1, convert_to_tensor=True)\n",
        "            embeddings2 = model.encode(texts2, convert_to_tensor=True)\n",
        "            # Calculate cosine similarities between pairs of embeddings\n",
        "            cosine_scores = torch.nn.functional.cosine_similarity(embeddings1, embeddings2)\n",
        "            # Apply a threshold to determine binary outcomes\n",
        "            threshold = 0.5\n",
        "            batch_predictions = (cosine_scores > threshold).type(torch.int)\n",
        "            predictions.extend(batch_predictions.tolist())\n",
        "\n",
        "    # Save the predictions to a CSV file\n",
        "    predictions_df = pd.DataFrame(predictions, columns=['prediction'])\n",
        "    predictions_df.to_csv(filepath, index=False)\n"
      ],
      "metadata": {
        "id": "6-e1grB7ZDsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test data"
      ],
      "metadata": {
        "id": "4UXr7N-6yzIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict test.csv\n",
        "test_dataset = load_dataset(base_url+'test.csv')\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16, collate_fn=custom_collate_fn)\n",
        "predict_and_save(model, test_dataloader, 'test_predictions.csv')"
      ],
      "metadata": {
        "id": "OShY3_S2e1DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dev data\n",
        "To confirm the model was loaded correctly"
      ],
      "metadata": {
        "id": "cJGoO2I4y2OK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader):\n",
        "    \"\"\"\n",
        "    Evaluate the SentenceTransformer model on a given dataset.\n",
        "\n",
        "    Args:\n",
        "        model (SentenceTransformer): The model to evaluate.\n",
        "        dataloader (DataLoader): A DataLoader containing the dataset for evaluation.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    labels = []\n",
        "\n",
        "    # Disable gradient calculations for efficiency and safety during inference.\n",
        "    with torch.no_grad():\n",
        "        for texts1, texts2, batch_labels in dataloader:\n",
        "            # Encode the pairs of texts to get their embeddings.\n",
        "            embeddings1 = model.encode(texts1, convert_to_tensor=True)\n",
        "            embeddings2 = model.encode(texts2, convert_to_tensor=True)\n",
        "\n",
        "            # Calculate the cosine similarity between pairs of embeddings.\n",
        "            cosine_scores = torch.nn.functional.cosine_similarity(embeddings1, embeddings2)\n",
        "\n",
        "            # Threshold the cosine scores to obtain binary predictions (0 or 1).\n",
        "            threshold = 0.5\n",
        "            batch_predictions = (cosine_scores > threshold).type(torch.int)\n",
        "            predictions.extend(batch_predictions.tolist())\n",
        "            labels.extend(batch_labels.tolist())\n",
        "\n",
        "    # Compute classification metrics\n",
        "    mcc = matthews_corrcoef(labels, predictions)\n",
        "    roc_auc = roc_auc_score(labels, predictions)\n",
        "\n",
        "    # Calculate confusion matrix to find TN, FP, FN, TP\n",
        "    tn, fp, fn, tp = confusion_matrix(labels, predictions).ravel()\n",
        "\n",
        "    # Calculate specificity and false positive rate\n",
        "    specificity = tn / (tn + fp)\n",
        "    false_positive_rate = fp / (tn + fp)\n",
        "\n",
        "    # Print all metrics\n",
        "    print(f\"Matthew's Correlation Coefficient: {mcc}\")\n",
        "    print(f\"ROC-AUC Score: {roc_auc}\")\n",
        "    print(f\"Specificity: {specificity}\")\n",
        "    print(f\"False Positive Rate: {false_positive_rate}\")\n",
        "\n",
        "    report = classification_report(labels, predictions, target_names=['Different Authors', 'Same Authors'])\n",
        "    print(\"Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "id": "n9M0hJsC8A17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate dev.csv with labels\n",
        "dev_dataset = load_dataset(base_url+'dev.csv')\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=16, collate_fn=custom_collate_fn)\n",
        "evaluate_model(model, dev_dataloader)\n",
        "# predict_and_save(model, dev_dataloader, 'dev_predictions.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tAj90DuZGC_",
        "outputId": "83f02178-6ac4-470b-e434-824f2f1b7900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthew's Correlation Coefficient: 0.6204283713329909\n",
            "ROC-AUC Score: 0.8097692202306276\n",
            "Specificity: 0.8377383740381399\n",
            "False Positive Rate: 0.16226162596186017\n",
            "Classification Report:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "Different Authors       0.79      0.84      0.81      2989\n",
            "     Same Authors       0.83      0.78      0.80      3011\n",
            "\n",
            "         accuracy                           0.81      6000\n",
            "        macro avg       0.81      0.81      0.81      6000\n",
            "     weighted avg       0.81      0.81      0.81      6000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}